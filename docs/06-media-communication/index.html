<!doctype html><html lang=en dir=ltr><head><meta name=generator content="Hugo 0.74.3"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Media Communication #  What do I get from WebRTC&rsquo;s media communication? #  WebRTC allows you to send and receive an unlimited amount of audio and video streams. You can add and remove these streams at anytime during a call. These streams could all be independent, or they could be bundled together! You could send a video feed of your desktop, and then include audio and video from your webcam."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Media Communication"><meta property="og:description" content="Media Communication #  What do I get from WebRTC&rsquo;s media communication? #  WebRTC allows you to send and receive an unlimited amount of audio and video streams. You can add and remove these streams at anytime during a call. These streams could all be independent, or they could be bundled together! You could send a video feed of your desktop, and then include audio and video from your webcam."><meta property="og:type" content="article"><meta property="og:url" content="https://webrtcforthecurious.com/docs/06-media-communication/"><meta property="article:modified_time" content="2022-02-24T14:00:49+08:00"><meta property="og:site_name" content="WebRTC for the Curious"><title>Media Communication | WebRTC for the Curious</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=alternate hreflang=ru href=https://webrtcforthecurious.com/ru/docs/06-media-communication/ title="Media Communication"><link rel=alternate hreflang=sv href=https://webrtcforthecurious.com/sv/docs/06-media-communication/ title=Mediakommunikation><link rel=alternate hreflang=zh href=https://webrtcforthecurious.com/zh/docs/06-media-communication/ title=媒体通信><link rel=alternate hreflang=ja href=https://webrtcforthecurious.com/ja/docs/06-media-communication/ title=メディア・コミュニケーション><link rel=alternate hreflang=fa href=https://webrtcforthecurious.com/fa/docs/06-media-communication/ title="ارتباط رسانه ای"><link rel=alternate hreflang=fr href=https://webrtcforthecurious.com/fr/docs/06-media-communication/ title="Media Communication"><link rel=alternate hreflang=id href=https://webrtcforthecurious.com/id/docs/06-media-communication/ title="Media Communication"><link rel=alternate hreflang=es href=https://webrtcforthecurious.com/es/docs/06-media-communication/ title="Media Communication"><link rel=stylesheet href=/book.min.6c7c6446dfdee7c8c933e9bbc6e80ee3ed6c913b2a59519f2092c3c6a9d63e55.css integrity="sha256-bHxkRt/e58jJM+m7xugO4+1skTsqWVGfIJLDxqnWPlU="><script defer src=/en.search.min.4eb49b7d5ffff7f6a97e815dfbba3e57fbe25eda4442d6d7710acba36c4c945e.js integrity="sha256-TrSbfV//9/apfoFd+7o+V/viXtpEQtbXcQrLo2xMlF4="></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a href=/><span>WebRTC for the Curious</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=https://webrtcforthecurious.com/docs/01-what-why-and-how/>What, Why and How</a></li><li><a href=https://webrtcforthecurious.com/docs/02-signaling/>Signaling</a></li><li><a href=https://webrtcforthecurious.com/docs/03-connecting/>Connecting</a></li><li><a href=https://webrtcforthecurious.com/docs/04-securing/>Securing</a></li><li><a href=https://webrtcforthecurious.com/docs/05-real-time-networking/>Real-time Networking</a></li><li><a href=https://webrtcforthecurious.com/docs/06-media-communication/ class=active>Media Communication</a></li><li><a href=https://webrtcforthecurious.com/docs/07-data-communication/>Data Communication</a></li><li><a href=https://webrtcforthecurious.com/docs/08-applied-webrtc/>Applied WebRTC</a></li><li><a href=https://webrtcforthecurious.com/docs/09-debugging/>Debugging</a></li><li><a href=https://webrtcforthecurious.com/docs/10-history-of-webrtc/>History</a></li><li><a href=https://webrtcforthecurious.com/docs/11-faq/>FAQ</a></li><li><a href=https://webrtcforthecurious.com/docs/12-glossary/>Glossary</a></li><li><a href=https://webrtcforthecurious.com/docs/13-reference/>Reference</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Media Communication</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#what-do-i-get-from-webrtcs-media-communication>What do I get from WebRTC&rsquo;s media communication?</a></li><li><a href=#how-does-it-work>How does it work?</a></li><li><a href=#latency-vs-quality>Latency vs Quality</a><ul><li><a href=#real-world-limitations>Real World Limitations</a></li><li><a href=#video-is-complex>Video is Complex</a></li></ul></li><li><a href=#video-101>Video 101</a><ul><li><a href=#lossy-and-lossless-compression>Lossy and Lossless compression</a></li><li><a href=#intra-and-inter-frame-compression>Intra and Inter frame compression</a></li><li><a href=#inter-frame-types>Inter-frame types</a></li><li><a href=#video-is-delicate>Video is delicate</a></li></ul></li><li><a href=#rtp>RTP</a><ul><li><a href=#packet-format>Packet Format</a></li><li><a href=#extensions>Extensions</a></li></ul></li><li><a href=#rtcp>RTCP</a><ul><li><a href=#packet-format-1>Packet Format</a></li><li><a href=#full-intra-frame-request-fir-and-picture-loss-indication-pli>Full INTRA-frame Request (FIR) and Picture Loss Indication (PLI)</a></li><li><a href=#negative-acknowledgment>Negative Acknowledgment</a></li><li><a href=#sender-and-receiver-reports>Sender and Receiver Reports</a></li></ul></li><li><a href=#how-rtprtcp-solve-problems-together>How RTP/RTCP solve problems together</a><ul><li><a href=#forward-error-correction>Forward Error Correction</a></li><li><a href=#adaptive-bitrate-and-bandwidth-estimation>Adaptive Bitrate and Bandwidth Estimation</a></li></ul></li><li><a href=#identifying-and-communicating-network-status>Identifying and Communicating Network Status</a><ul><li><a href=#receiver-reports--sender-reports>Receiver Reports / Sender Reports</a></li><li><a href=#tmmbr-tmmbn-remb-and-twcc-paired-with-gcc>TMMBR, TMMBN, REMB and TWCC, paired with GCC</a></li></ul></li><li><a href=#bandwidth-estimation-alternatives>Bandwidth Estimation Alternatives</a></li></ul></nav></aside></header><article class=markdown><h1 id=media-communication>Media Communication
<a class=anchor href=#media-communication>#</a></h1><h2 id=what-do-i-get-from-webrtcs-media-communication>What do I get from WebRTC&rsquo;s media communication?
<a class=anchor href=#what-do-i-get-from-webrtcs-media-communication>#</a></h2><p>WebRTC allows you to send and receive an unlimited amount of audio and video streams. You can add and remove these streams at anytime during a call. These streams could all be independent, or they could be bundled together! You could send a video feed of your desktop, and then include audio and video from your webcam.</p><p>The WebRTC protocol is codec agnostic. The underlying transport supports everything, even things that don&rsquo;t exist yet! However, the WebRTC Agent you are communicating with may not have the necessary tools to accept it.</p><p>WebRTC is also designed to handle dynamic network conditions. During a call your bandwidth might increase, or decrease. Maybe you suddenly experience lots of packet loss. The protocol is designed to handle all of this. WebRTC responds to network conditions and tries to give you the best experience possible with the resources available.</p><h2 id=how-does-it-work>How does it work?
<a class=anchor href=#how-does-it-work>#</a></h2><p>WebRTC uses two preexisting protocols RTP and RTCP, both defined in <a href=https://tools.ietf.org/html/rfc1889>RFC 1889</a>.</p><p>RTP (Real-time Transport Protocol) is the protocol that carries the media. It was designed to allow for real-time delivery of video. It does not stipulate any rules around latency or reliability, but gives you the tools to implement them. RTP gives you streams, so you can run multiple media feeds over one connection. It also gives you the timing and ordering information you need to feed a media pipeline.</p><p>RTCP (RTP Control Protocol) is the protocol that communicates metadata about the call. The format is very flexible and allows you to add any metadata you want. This is used to communicate statistics about the call. It is also used to handle packet loss and to implement congestion control. It gives you the bi-directional communication necessary to respond to changing network conditions.</p><h2 id=latency-vs-quality>Latency vs Quality
<a class=anchor href=#latency-vs-quality>#</a></h2><p>Real-time media is about making trade-offs between latency and quality. The more latency you are willing to tolerate, the higher quality video you can expect.</p><h3 id=real-world-limitations>Real World Limitations
<a class=anchor href=#real-world-limitations>#</a></h3><p>These constraints are all caused by the limitations of the real world. They are all characteristics of your network that you will need to overcome.</p><h3 id=video-is-complex>Video is Complex
<a class=anchor href=#video-is-complex>#</a></h3><p>Transporting video isn&rsquo;t easy. To store 30 minutes of uncompressed 720 8-bit video you need about 110 GB. With those numbers, a 4-person conference call isn&rsquo;t going to happen. We need a way to make it smaller, and the answer is video compression. That doesn&rsquo;t come without downsides though.</p><h2 id=video-101>Video 101
<a class=anchor href=#video-101>#</a></h2><p>We aren&rsquo;t going to cover video compression in depth, but just enough to understand why RTP is designed the way it is. Video compression encodes video into a new format that requires fewer bits to represent the same video.</p><h3 id=lossy-and-lossless-compression>Lossy and Lossless compression
<a class=anchor href=#lossy-and-lossless-compression>#</a></h3><p>You can encode video to be lossless (no information is lost) or lossy (information may be lost). Because lossless encoding requires more data to be sent to a peer, making for a higher latency stream and more dropped packets, RTP typically uses lossy compression even though the video quality won’t be as good.</p><h3 id=intra-and-inter-frame-compression>Intra and Inter frame compression
<a class=anchor href=#intra-and-inter-frame-compression>#</a></h3><p>Video compression comes in two types. The first is intra-frame. Intra-frame compression reduces the bits used to describe a single video frame. The same techniques are used to compress still pictures, like the JPEG compression method.</p><p>The second type is inter-frame compression. Since video is made up of many pictures we look for ways to not send the same information twice.</p><h3 id=inter-frame-types>Inter-frame types
<a class=anchor href=#inter-frame-types>#</a></h3><p>You then have three frame types:</p><ul><li><strong>I-Frame</strong> - A complete picture, can be decoded without anything else.</li><li><strong>P-Frame</strong> - A partial picture, containing only changes from the previous picture.</li><li><strong>B-Frame</strong> - A partial picture, is a modification of previous and future pictures.</li></ul><p>The following is visualization of the three frame types.</p><p><img src=../images/06-frame-types.png alt="Frame types" title="Frame types"></p><h3 id=video-is-delicate>Video is delicate
<a class=anchor href=#video-is-delicate>#</a></h3><p>Video compression is incredibly stateful, making it difficult to transfer over the internet. What happens If you lose part of an I-Frame? How does a P-Frame know what to modify? As video compression gets more complex, this is becoming even more of a problem. Luckily RTP and RTCP have the solution.</p><h2 id=rtp>RTP
<a class=anchor href=#rtp>#</a></h2><h3 id=packet-format>Packet Format
<a class=anchor href=#packet-format>#</a></h3><p>Every RTP packet has the following structure:</p><pre><code> 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|V=2|P|X|  CC   |M|     PT      |       Sequence Number         |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                           Timestamp                           |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|           Synchronization Source (SSRC) identifier            |
+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
|            Contributing Source (CSRC) identifiers             |
|                             ....                              |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                            Payload                            |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
</code></pre><h4 id=version-v>Version (V)
<a class=anchor href=#version-v>#</a></h4><p><code>Version</code> is always <code>2</code></p><h4 id=padding-p>Padding (P)
<a class=anchor href=#padding-p>#</a></h4><p><code>Padding</code> is a bool that controls if the payload has padding.</p><p>The last byte of the payload contains a count of how many padding bytes were added.</p><h4 id=extension-x>Extension (X)
<a class=anchor href=#extension-x>#</a></h4><p>If set, the RTP header will have extensions. This is described in greater detail below.</p><h4 id=csrc-count-cc>CSRC count (CC)
<a class=anchor href=#csrc-count-cc>#</a></h4><p>The amount of <code>CSRC</code> identifiers that follow after the <code>SSRC</code>, and before the payload.</p><h4 id=marker-m>Marker (M)
<a class=anchor href=#marker-m>#</a></h4><p>The marker bit has no pre-set meaning, and can be used however the user likes.</p><p>In some cases it is set when a user is speaking. It is also commonly used to mark a keyframe.</p><h4 id=payload-type-pt>Payload Type (PT)
<a class=anchor href=#payload-type-pt>#</a></h4><p><code>Payload Type</code> is a unique identifier for what codec is being carried by this packet.</p><p>For WebRTC the <code>Payload Type</code> is dynamic. VP8 in one call may be different from another. The offerer in the call determines the mapping of <code>Payload Types</code> to codecs in the <code>Session Description</code>.</p><h4 id=sequence-number>Sequence Number
<a class=anchor href=#sequence-number>#</a></h4><p><code>Sequence Number</code> is used for ordering packets in a stream. Every time a packet is sent the <code>Sequence Number</code> is incremented by one.</p><p>RTP is designed to be useful over lossy networks. This gives the receiver a way to detect when packets have been lost.</p><h4 id=timestamp>Timestamp
<a class=anchor href=#timestamp>#</a></h4><p>The sampling instant for this packet. This is not a global clock, but how much time has passed in the media stream. Several RTP packets can have the same timestamp if they for example are all part of the same video frame.</p><h4 id=synchronization-source-ssrc>Synchronization Source (SSRC)
<a class=anchor href=#synchronization-source-ssrc>#</a></h4><p>An <code>SSRC</code> is the unique identifier for this stream. This allows you to run multiple streams of media over a single RTP stream.</p><h4 id=contributing-source-csrc>Contributing Source (CSRC)
<a class=anchor href=#contributing-source-csrc>#</a></h4><p>A list that communicates what <code>SSRC</code>es contributed to this packet.</p><p>This is commonly used for talking indicators. Let&rsquo;s say server side you combined multiple audio feeds into a single RTP stream. You could then use this field to say &ldquo;Input stream A and C were talking at this moment&rdquo;.</p><h4 id=payload>Payload
<a class=anchor href=#payload>#</a></h4><p>The actual payload data. Might end with the count of how many padding bytes were added, if the padding flag is set.</p><h3 id=extensions>Extensions
<a class=anchor href=#extensions>#</a></h3><h2 id=rtcp>RTCP
<a class=anchor href=#rtcp>#</a></h2><h3 id=packet-format-1>Packet Format
<a class=anchor href=#packet-format-1>#</a></h3><p>Every RTCP packet has the following structure:</p><pre><code> 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|V=2|P|    RC   |       PT      |             length            |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                            Payload                            |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
</code></pre><h4 id=version-v-1>Version (V)
<a class=anchor href=#version-v-1>#</a></h4><p><code>Version</code> is always <code>2</code>.</p><h4 id=padding-p-1>Padding (P)
<a class=anchor href=#padding-p-1>#</a></h4><p><code>Padding</code> is a bool that controls if the payload has padding.</p><p>The last byte of the payload contains a count of how many padding bytes were added.</p><h4 id=reception-report-count-rc>Reception Report Count (RC)
<a class=anchor href=#reception-report-count-rc>#</a></h4><p>The number of reports in this packet. A single RTCP packet can contain multiple events.</p><h4 id=packet-type-pt>Packet Type (PT)
<a class=anchor href=#packet-type-pt>#</a></h4><p>Unique Identifier for what type of RTCP Packet this is. A WebRTC Agent doesn&rsquo;t need to support all these types, and support between Agents can be different. These are the ones you may commonly see though:</p><ul><li><code>192</code> - Full INTRA-frame Request (<code>FIR</code>)</li><li><code>193</code> - Negative ACKnowledgements (<code>NACK</code>)</li><li><code>200</code> - Sender Report</li><li><code>201</code> - Receiver Report</li><li><code>205</code> - Generic RTP Feedback</li><li><code>206</code> - Payload Specific Feedback</li></ul><p>The significance of these packet types will be described in greater detail below.</p><h3 id=full-intra-frame-request-fir-and-picture-loss-indication-pli>Full INTRA-frame Request (FIR) and Picture Loss Indication (PLI)
<a class=anchor href=#full-intra-frame-request-fir-and-picture-loss-indication-pli>#</a></h3><p>Both <code>FIR</code> and <code>PLI</code> messages serve a similar purpose. These messages request a full key frame from the sender.
<code>PLI</code> is used when partial frames were given to the decoder, but it was unable to decode them.
This could happen because you had lots of packet loss, or maybe the decoder crashed.</p><p>According to <a href=https://tools.ietf.org/html/rfc5104#section-4.3.1.2>RFC 5104</a>, <code>FIR</code> shall not be used when packets or frames are lost. That is <code>PLI</code>s job. <code>FIR</code> requests a key frame for reasons other than packet loss - for example when a new member enters a video conference. They need a full key frame to start decoding video stream, the decoder will be discarding frames until key frame arrives.</p><p>It is a good idea for a receiver to request a full key frame right after connecting, this minimizes the delay between connecting, and an image showing up on the user&rsquo;s screen.</p><p><code>PLI</code> packets are a part of Payload Specific Feedback messages.</p><p>In practice, software that is able to handle both <code>PLI</code> and <code>FIR</code> packets will act the same way in both cases. It will send a signal to the encoder to produce a new full key frame.</p><h3 id=negative-acknowledgment>Negative Acknowledgment
<a class=anchor href=#negative-acknowledgment>#</a></h3><p>A <code>NACK</code> requests that a sender re-transmits a single RTP packet. This is usually caused by an RTP packet getting lost, but could also happen because it is late.</p><p><code>NACK</code>s are much more bandwidth efficient than requesting that the whole frame get sent again. Since RTP breaks up packets into very small chunks, you are really just requesting one small missing piece. The receiver crafts an RTCP message with the SSRC and Sequence Number. If the sender does not have this RTP packet available to re-send, it just ignores the message.</p><h3 id=sender-and-receiver-reports>Sender and Receiver Reports
<a class=anchor href=#sender-and-receiver-reports>#</a></h3><p>These reports are used to send statistics between agents. This communicates the amount of packets actually received and jitter.</p><p>The reports can be used for diagnostics and congestion control.</p><h2 id=how-rtprtcp-solve-problems-together>How RTP/RTCP solve problems together
<a class=anchor href=#how-rtprtcp-solve-problems-together>#</a></h2><p>RTP and RTCP then work together to solve all the problems caused by networks. These techniques are still constantly changing!</p><h3 id=forward-error-correction>Forward Error Correction
<a class=anchor href=#forward-error-correction>#</a></h3><p>Also known as FEC. Another method of dealing with packet loss. FEC is when you send the same data multiple times, without it even being requested. This is done at the RTP level, or even lower with the codec.</p><p>If the packet loss for a call is steady then FEC is a much lower latency solution than NACK. The round trip time of having to request, and then re-transmit the missing packet can be significant for NACKs.</p><h3 id=adaptive-bitrate-and-bandwidth-estimation>Adaptive Bitrate and Bandwidth Estimation
<a class=anchor href=#adaptive-bitrate-and-bandwidth-estimation>#</a></h3><p>As discussed in the <a href=../05-real-time-networking/>Real-time networking</a> chapter, networks are unpredictable and unreliable. Bandwidth availability can change multiple times throughout a session.
It is not uncommon to see available bandwidth change dramatically (orders of magnitude) within a second.</p><p>The main idea is to adjust encoding bitrate based on predicted, current, and future available network bandwidth.
This ensures that video and audio signal of the best possible quality is transmitted, and the connection does not get dropped because of network congestion.
Heuristics that model the network behavior and tries to predict it is known as Bandwidth estimation.</p><p>There is a lot of nuance to this, so let&rsquo;s explore in greater detail.</p><h2 id=identifying-and-communicating-network-status>Identifying and Communicating Network Status
<a class=anchor href=#identifying-and-communicating-network-status>#</a></h2><p>RTP/RTCP runs over all types of different networks, and as a result, it&rsquo;s common for some
communication to be dropped on its way from the sender to the receiver. Being built on top of UDP,
there is no built-in mechanism for packet retransmission, let alone handling congestion control.</p><p>To provide users the best experience, WebRTC must estimate qualities about the network path, and
adapt to how those qualities change over time. The key traits to monitor include: available
bandwidth (in each direction, as it may not be symmetric), round trip time, and jitter (fluctuations
in round trip time). It needs to account for packet loss, and communicate changes in these
properties as network conditions evolve.</p><p>There are two primary objectives for these protocols:</p><ol><li>Estimate the available bandwidth (in each direction) supported by the network.</li><li>Communicate network characteristics between sender and receiver.</li></ol><p>RTP/RTCP has three different approaches to address this problem. They all have their pros and cons,
and generally each generation has improved over its predecessors. Which implementation you use will
depend primarily on the software stack available to your clients and the libraries available for
building your application.</p><h3 id=receiver-reports--sender-reports>Receiver Reports / Sender Reports
<a class=anchor href=#receiver-reports--sender-reports>#</a></h3><p>The first implementation is the pair of Receiver Reports and its complement, Sender Reports. These
RTCP messages are defined in <a href=https://tools.ietf.org/html/rfc3550#section-6.4>RFC 3550</a>, and are
responsible for communicating network status between endpoints. Receiver Reports focuses on
communicating qualities about the network (including packet loss, round-trip time, and jitter), and
it pairs with other algorithms that are then responsible for estimating available bandwidth based on
these reports.</p><p>Sender and Receiver reports (SR and RR) together paint a picture of the network quality. They are
sent on a schedule for each SSRC, and they are the inputs used when estimating available
bandwidth. Those estimates are made by the sender after receiving the RR data, containing the
following fields:</p><ul><li><strong>Fraction Lost</strong> - What percentage of packets have been lost since the last Receiver Report.</li><li><strong>Cumulative Number of Packets Lost</strong> - How many packets have been lost during the entire call.</li><li><strong>Extended Highest Sequence Number Received</strong> - What was the last Sequence Number received, and
how many times has it rolled over.</li><li><strong>Interarrival Jitter</strong> - The rolling Jitter for the entire call.</li><li><strong>Last Sender Report Timestamp</strong> - Last known time on sender, used for round-trip time
calculation.</li></ul><p>SR and RR work together to compute round-trip time.</p><p>The sender includes its local time, <code>sendertime1</code> in SR. When the receiver gets an SR packet, it
sends back RR. Among other things, the RR includes <code>sendertime1</code> just received from the sender.
There will be a delay between receiving the SR and sending the RR. Because of that, the RR also
includes a &ldquo;delay since last sender report&rdquo; time - <code>DLSR</code>. The <code>DLSR</code> is used to adjust the
round-trip time estimate later on in the process. Once the sender receives the RR it subtracts
<code>sendertime1</code> and <code>DLSR</code> from the current time <code>sendertime2</code>. This time delta is called round-trip
propagation delay or round-trip time.</p><p><code>rtt = sendertime2 - sendertime1 - DLSR</code></p><p>Round-trip time in plain English:</p><ul><li>I send you a message with my clock&rsquo;s current reading, say it is 4:20pm, 42 seconds and 420 milliseconds.</li><li>You send me this same timestamp back.</li><li>You also include the time elapsed from reading my message to sending the message back, say 5 milliseconds.</li><li>Once I receive the time back, I look at the clock again.</li><li>Now my clock says 4:20pm, 42 seconds 690 milliseconds.</li><li>It means that it took 265 milliseconds (690 - 420 - 5) to reach you and return back to me.</li><li>Therefore, the round-trip time is 265 milliseconds.</li></ul><p><img src=../images/06-rtt.png alt="Round-trip time" title="Round-trip time"></p><h3 id=tmmbr-tmmbn-remb-and-twcc-paired-with-gcc>TMMBR, TMMBN, REMB and TWCC, paired with GCC
<a class=anchor href=#tmmbr-tmmbn-remb-and-twcc-paired-with-gcc>#</a></h3><h4 id=google-congestion-control-gcc>Google Congestion Control (GCC)
<a class=anchor href=#google-congestion-control-gcc>#</a></h4><p>The Google Congestion Control (GCC) algorithm (outlined in
<a href=https://tools.ietf.org/html/draft-ietf-rmcat-gcc-02>draft-ietf-rmcat-gcc-02</a>) addresses the
challenge of bandwidth estimation. It pairs with a variety of other protocols to facilitate the
associated communication requirements. Consequently, it is well-suited to run on either the
receiving side (when run with TMMBR/TMMBN or REMB) or on the sending side (when run with TWCC).</p><p>To arrive at estimates for available bandwidth, GCC focuses on packet loss and fluctuations in frame
arrival time as its two primary metrics. It runs these metrics through two linked controllers: the
loss-based controller and the delay-based controller.</p><p>GCC&rsquo;s first component, the loss-based controller, is simple:</p><ul><li>If packet loss is above 10%, the bandwidth estimate is reduced.</li><li>If packet loss is between 2-10%, the bandwidth estimate stays the same.</li><li>If packet loss is below 2%, the bandwidth estimate is increased.</li></ul><p>Packet loss measurements are taken frequently. Depending on the paired communication protocol,
packet loss may be either be explicitly communicated (as with TWCC) or inferred (as with TMMBR/TMMBN
and REMB). These percentages are evaluated over time windows of around one second.</p><p>The second function cooperates with the loss-based controller, and looks at the variations in packet
arrival time. This delay-based controller aims to identify when network links are becoming
increasingly congested, and may reduce bandwidth estimates even before packet loss occurs. The
theory is that the busiest network interface along the path will continue queuing up packets up
until the interface runs out of capacity inside its buffers. If that interface continues to receive
more traffic than it is able to send, it will be forced to drop all packets that it cannot fit into
its buffer space. This type of packet loss is particularly disruptive for low-latency/real-time
communication, but it can also degrade throughput for all communication over that link and should
ideally be avoided. Thus, GCC tries to figure out if network links are growing larger and larger
queue depths <em>before</em> packet loss actually occurs. It will reduce the bandwidth usage if it observes
increased queuing delays over time.</p><p>To achieve this, GCC tries to infer increases in queue depth by measuring subtle increases in round
trip time. It records frames&rsquo; &ldquo;inter-arrival time&rdquo;, <code>t(i) - t(i-1)</code>: the difference in arrival time
of two groups of packets (generally, consecutive video frames). These packet groups frequently
depart at regular time intervals (e.g. every 1/24 seconds for a 24 fps video). As a result,
measuring inter-arrival time is then as simple as recording the time difference between the start of
the first packet group (i.e. frame) and the first frame of the next.</p><p>In the diagram below, the median inter-packet delay increase is +20 msec, a clear indicator of
network congestion.</p><p><img src=../images/06-twcc.png alt="TWCC with delay" title="TWCC with delay"></p><p>If inter-arrival time increases over time, that is presumed evidence of increased queue depth on
connecting network interfaces and considered to be network congestion. (Note: GCC is smart enough to
control these measurements for fluctuations in frame byte sizes.) GCC refines its latency
measurements using a <a href=https://en.wikipedia.org/wiki/Kalman_filter>Kalman filter</a> and takes many
measurements of network round-trip times (and its variations) before flagging congestion. One can
think of GCC&rsquo;s Kalman filter as taking the place of a linear regression: helping to make accurate
predictions even when jitter adds noise into the timing measurements. Upon flagging congestion, GCC
will reduce the available bitrate. Alternatively, under steady network conditions, it can slowly
increase its bandwidth estimates to test out higher load values.</p><h4 id=tmmbr-tmmbn-and-remb>TMMBR, TMMBN, and REMB
<a class=anchor href=#tmmbr-tmmbn-and-remb>#</a></h4><p>For TMMBR/TMMBN and REMB, the receiving side first estimates available inbound bandwidth (using a
protocol such as GCC), and then communicates these bandwidth estimates to the remote senders. They
do not need to exchange details about packet loss or other qualities about network congestion
because operating on the receiving side allows them to measure inter-arrival time and packet loss
directly. Instead, TMMBR, TMMBN, and REMB exchange just the bandwidth estimates themselves:</p><ul><li><strong>Temporary Maximum Media Stream Bit Rate Request</strong> - A mantissa/exponent of a requested bitrate
for a single SSRC.</li><li><strong>Temporary Maximum Media Stream Bit Rate Notification</strong> - A message to notify that a TMMBR has
been received.</li><li><strong>Receiver Estimated Maximum Bitrate</strong> - A mantissa/exponent of a requested bitrate for the
entire session.</li></ul><p>TMMBR and TMMBN came first and are defined in <a href=https://tools.ietf.org/html/rfc5104>RFC 5104</a>. REMB
came later, there was a draft submitted in
<a href=https://tools.ietf.org/html/draft-alvestrand-rmcat-remb-03>draft-alvestrand-rmcat-remb</a>, but it
was never standardized.</p><p>An example session that uses REMB might behave like the following:</p><p><img src=../images/06-remb.png alt=REMB title=REMB></p><p>This method works great on paper. The Sender receives estimation from the receiver, sets encoder bitrate to the received value. Tada! We&rsquo;ve adjusted to the network conditions.</p><p>However in practice, the REMB approach has multiple drawbacks.</p><p>Encoder inefficiency is the first. When you set a bitrate for the encoder, it won&rsquo;t necessarily
output the exact bitrate you requested. Encoding may output more or fewer bits, depending on the
encoder settings and the frame being encoded.</p><p>For example, using the x264 encoder with <code>tune=zerolatency</code> can significantly deviate from the specified target bitrate. Here is a possible scenario:</p><ul><li>Let&rsquo;s say we start off by setting the bitrate to 1000 kbps.</li><li>The encoder outputs only 700 kbps, because there is not enough high frequency features to encode. (AKA - &ldquo;staring at a wall&rdquo;.)</li><li>Let&rsquo;s also imagine that the receiver gets the 700 kbps video at zero packet loss. It then applies REMB rule 1 to increase the incoming bitrate by 8%.</li><li>The receiver sends a REMB packet with a 756 kbps suggestion (700 kbps * 1.08) to the sender.</li><li>The sender sets the encoder bitrate to 756 kbps.</li><li>The encoder outputs an even lower bitrate.</li><li>This process continues to repeat itself, lowering the bitrate to the absolute minimum.</li></ul><p>You can see how this would cause heavy encoder parameter tuning, and surprise users with unwatchable video even on a great connection.</p><h4 id=transport-wide-congestion-control>Transport Wide Congestion Control
<a class=anchor href=#transport-wide-congestion-control>#</a></h4><p>Transport Wide Congestion Control is the latest development in RTCP network status
communication. It is defined in
<a href=https://datatracker.ietf.org/doc/html/draft-holmer-rmcat-transport-wide-cc-extensions-01>draft-holmer-rmcat-transport-wide-cc-extensions-01</a>,
but has also never been standardized.</p><p>TWCC uses a quite simple principle:</p><p><img src=../images/06-twcc-idea.png alt=TWCC title=TWCC></p><p>With REMB, the receiver instructs the sending side in the available download bitrate. It uses
precise measurements about inferred packet loss and data only it has about inter-packet arrival
time.</p><p>TWCC is almost a hybrid approach between the SR/RR and REMB generations of protocols. It brings the
bandwidth estimates back to the sender side (similar to SR/RR), but its bandwidth estimate technique
more closely resembles the REMB generation.</p><p>With TWCC, the receiver lets the sender know the arrival time of each packet. This is enough
information for the sender to measure inter-packet arrival delay variation, as well as identifying
which packets were dropped or arrived too late to contribute to the audio/video feed. With this data
being exchanged frequently, the sender able to quickly adjust to changing network conditions and
vary its output bandwidth using an algorithm such GCC.</p><p>The sender keeps track of sent packets, their sequence numbers, sizes and timestamps. When the
sender receives RTCP messages from the receiver, it compares the send inter-packet delays with
the receive delays. If the receive delays increase, it signals network congestion, and the sender
must take corrective measures.</p><p>By providing the sender with the raw data, TWCC provides an excellent view into real time network
conditions:</p><ul><li>Almost instant packet loss behavior, down to the individual lost packets</li><li>Accurate send bitrate</li><li>Accurate receive bitrate</li><li>Jitter measurement</li><li>Differences between send and receive packet delays</li><li>Description of how the network tolerated bursty or steady bandwidth delivery</li></ul><p>One of the most significant contributions of TWCC is the flexibility it affords to WebRTC
developers. By consolidating the congestion control algorithm to the sending side, it allows simple
client code that can be widely used and requires minimal enhancements over time. The complex
congestion control algorithms can then be iterated more quickly on the hardware they directly
control (like the Selective Forwarding Unit, discussed in section 8). In the case of browsers and
mobile devices, this means those clients can benefit from algorithm enhancements without having to
await standardization or browser updates (which can take quite a long time to be widely available).</p><h2 id=bandwidth-estimation-alternatives>Bandwidth Estimation Alternatives
<a class=anchor href=#bandwidth-estimation-alternatives>#</a></h2><p>The most deployed implementation is &ldquo;A Google Congestion Control Algorithm for Real-Time
Communication&rdquo; defined in
<a href=https://tools.ietf.org/html/draft-alvestrand-rmcat-congestion-02>draft-alvestrand-rmcat-congestion</a>.</p><p>There are several alternatives to GCC, for example <a href=https://tools.ietf.org/html/draft-zhu-rmcat-nada-04>NADA: A Unified Congestion Control Scheme for
Real-Time Media</a> and <a href=https://tools.ietf.org/html/draft-johansson-rmcat-scream-cc-05>SCReAM - Self-Clocked
Rate Adaptation for Multimedia</a>.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div class=book-languages tabindex=0 aria-haspopup=true><ul><li class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
English</li></ul><ul class=book-languages-list><li class=active><a href=https://webrtcforthecurious.com/ class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
English</a></li><li><a href=https://webrtcforthecurious.com/ru/docs/06-media-communication/ class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
Русский</a></li><li><a href=https://webrtcforthecurious.com/sv/docs/06-media-communication/ class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
Svenska</a></li><li><a href=https://webrtcforthecurious.com/zh/docs/06-media-communication/ class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
简体中文</a></li><li><a href=https://webrtcforthecurious.com/ja/docs/06-media-communication/ class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
日本語</a></li><li><a href=https://webrtcforthecurious.com/fa/docs/06-media-communication/ class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
Persian</a></li><li><a href=https://webrtcforthecurious.com/fr/docs/06-media-communication/ class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
Français</a></li><li><a href=https://webrtcforthecurious.com/id/docs/06-media-communication/ class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
Bahasa Indonesia</a></li><li><a href=https://webrtcforthecurious.com/es/docs/06-media-communication/ class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
Español</a></li></ul></div><div><a class="flex align-center" href=https://github.com/webrtc-for-the-curious/webrtc-for-the-curious/commit/d00d6dac5b790b409ada9ff4e117cff592a54e80 title="Last modified by Pang | February 24, 2022" target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 24, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/webrtc-for-the-curious/webrtc-for-the-curious/edit/master/content/docs/06-media-communication.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#what-do-i-get-from-webrtcs-media-communication>What do I get from WebRTC&rsquo;s media communication?</a></li><li><a href=#how-does-it-work>How does it work?</a></li><li><a href=#latency-vs-quality>Latency vs Quality</a><ul><li><a href=#real-world-limitations>Real World Limitations</a></li><li><a href=#video-is-complex>Video is Complex</a></li></ul></li><li><a href=#video-101>Video 101</a><ul><li><a href=#lossy-and-lossless-compression>Lossy and Lossless compression</a></li><li><a href=#intra-and-inter-frame-compression>Intra and Inter frame compression</a></li><li><a href=#inter-frame-types>Inter-frame types</a></li><li><a href=#video-is-delicate>Video is delicate</a></li></ul></li><li><a href=#rtp>RTP</a><ul><li><a href=#packet-format>Packet Format</a></li><li><a href=#extensions>Extensions</a></li></ul></li><li><a href=#rtcp>RTCP</a><ul><li><a href=#packet-format-1>Packet Format</a></li><li><a href=#full-intra-frame-request-fir-and-picture-loss-indication-pli>Full INTRA-frame Request (FIR) and Picture Loss Indication (PLI)</a></li><li><a href=#negative-acknowledgment>Negative Acknowledgment</a></li><li><a href=#sender-and-receiver-reports>Sender and Receiver Reports</a></li></ul></li><li><a href=#how-rtprtcp-solve-problems-together>How RTP/RTCP solve problems together</a><ul><li><a href=#forward-error-correction>Forward Error Correction</a></li><li><a href=#adaptive-bitrate-and-bandwidth-estimation>Adaptive Bitrate and Bandwidth Estimation</a></li></ul></li><li><a href=#identifying-and-communicating-network-status>Identifying and Communicating Network Status</a><ul><li><a href=#receiver-reports--sender-reports>Receiver Reports / Sender Reports</a></li><li><a href=#tmmbr-tmmbn-remb-and-twcc-paired-with-gcc>TMMBR, TMMBN, REMB and TWCC, paired with GCC</a></li></ul></li><li><a href=#bandwidth-estimation-alternatives>Bandwidth Estimation Alternatives</a></li></ul></nav></div></aside></main></body></html>